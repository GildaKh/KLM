{"paragraphs":[{"text":"//https://transform.now.sh/json-to-scala-case-class/\ncase class PassengersList (\n  uci: String,\n  passengerType: String,\n  tattoo: Int,\n  weight: Int,\n  category: String,\n  crid: Option[String]\n)\n\ncase class Flight (\n  marketingAirline: String,\n  marketingFlightNumber: String,\n  originAirport: String,\n  originTerminal: String,\n  destinationAirport: String,\n  destinationTerminal: String,\n  departureDate: String,\n  arrivalDate: String,\n  operatingAirline: String,\n  operatingFlightNumber: String\n)\n\ncase class ProductsList (\n  ptype: String,\n  tattoo: String,\n  bookingStatus: String,\n  bookingClass: String,\n  transportClass: String,\n  aircraftType: String,\n  nbPassengers: String,\n  yieldOrigin: String,\n  yieldDestination: String,\n  yieldTripOrigin: String,\n  yieldTripDestination: String,\n  yieldPointOfCommencement: String,\n  flight: Flight\n)\n\ncase class Travelrecord (\n  creationDate: String,\n  purgeDateAmd: String,\n  lastEotDate: String,\n  envelopNumber: Int,\n  nbPassengers: Int,\n  isMarketingBlockspace: Boolean,\n  isTechnicalLastUpdater: Boolean,\n  passengersList: Seq[PassengersList],\n  productsList: Seq[ProductsList]\n)\n\ncase class DataElement (\n  travelrecord: Travelrecord\n)\n\ncase class Event (\n  DataElement: DataElement\n)\n\ncase class RootInterface (\n  timestamp: String,\n  event: Event\n)","user":"anonymous","dateUpdated":"2019-07-11T13:31:39+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562712368117_289792814","id":"20190709-224608_1316476991","dateCreated":"2019-07-09T22:46:08+0000","dateStarted":"2019-07-11T13:31:39+0000","dateFinished":"2019-07-11T13:31:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:586"},{"title":"Write to Kafka Sample  ","text":"//if we assume that data is fed to cluster from outside. In this case kafka reads the data\r\nval df = spark.read.json(\"hdfs:///user/maria_dev/KLM/Sample.dat\").toDF()\r\n\r\n//Import data to kafka\r\nval ds = df\r\n  .writeStream\r\n  .format(\"kafka\")\r\n  .option(\"kafka.bootstrap.servers\", \"sandbox.hortonworks.com:5181\")\r\n  .option(\"topic\", \"booking\")\r\n  .start()","user":"anonymous","dateUpdated":"2019-07-10T21:15:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562762884688_-1021592743","id":"20190710-124804_1244739156","dateCreated":"2019-07-10T12:48:04+0000","dateStarted":"2019-07-10T14:53:30+0000","dateFinished":"2019-07-10T14:53:33+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:587"},{"title":"Read from Kafka ","text":"//if we assume that data is fed to cluster from outside. In this case, we read the data from kafka\r\n\r\nimport org.apache.spark.sql.catalyst.ScalaReflection\r\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};\r\nimport spark.implicits._\r\nimport org.apache.spark.sql.functions._\r\n\r\n//Booking JSON schema\r\nval schema = ScalaReflection.schemaFor[RootInterface].dataType.asInstanceOf[StructType]\r\n\r\n//Read data from kafka\r\nval df = spark\r\n  .readStream\r\n  .format(\"kafka\")\r\n  .option(\"kafka.bootstrap.servers\", \"sandbox.hortonworks.com:5181\")\r\n  .option(\"subscribe\", \"booking\")\r\n  //.option(\"startingOffsets\", \"\"\"{\"topic1\":{\"0\":23,\"1\":-2},\"topic2\":{\"0\":-2}}\"\"\")  //multiple topics, specifying explicit Kafka offsets\r\n  //.option(\"endingOffsets\", \"\"\"{\"topic1\":{\"0\":50,\"1\":-1},\"topic2\":{\"0\":-1}}\"\"\")\r\n  .load()\r\n","user":"anonymous","dateUpdated":"2019-07-10T18:16:13+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562757599881_1055854711","id":"20190710-111959_1403780165","dateCreated":"2019-07-10T11:19:59+0000","dateStarted":"2019-07-10T15:15:11+0000","dateFinished":"2019-07-10T15:15:03+0000","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:588"},{"title":"Streaming from a folder in HDFS","text":"\nimport org.apache.spark.sql.catalyst.ScalaReflection\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};\nimport org.apache.spark.sql.{Dataset, DataFrame}\n\n\ndef GetBookingDataFrameStreaming(uri:String) : org.apache.spark.sql.DataFrame = {\n    \n    val schema = ScalaReflection.schemaFor[RootInterface].dataType.asInstanceOf[StructType]\n    val df = spark.readStream.format(\"json\").schema(schema).load(uri)  //Read from directory\n\n\n                        \n    //fetch required columns\n    val details = df.withColumn(\"nbPassengers\", $\"event.DataElement.travelrecord.productsList.nbPassengers\")\n        .withColumn(\"passengerType\", $\"event.DataElement.travelrecord.passengersList.passengerType\")\n        .withColumn(\"bookingStatus\", $\"event.DataElement.travelrecord.productsList.bookingStatus\")\n        .withColumn(\"operatingAirline\", $\"event.DataElement.travelrecord.productsList.flight.operatingAirline\")\n        .withColumn(\"originAirport\", $\"event.DataElement.travelrecord.productsList.flight.originAirport\")\n        .withColumn(\"destinationAirport\", $\"event.DataElement.travelrecord.productsList.flight.destinationAirport\")\n        .withColumn(\"departureDate\", $\"event.DataElement.travelrecord.productsList.flight.departureDate\")\n        .withColumn(\"arrivalDate\", $\"event.DataElement.travelrecord.productsList.flight.arrivalDate\").drop(\"event\")\n                \n                        \n    //split array columns into separate columns\n    val booking = details.withColumn(\"temp\", col(\"nbPassengers\")).select(\n            col(\"*\") +:(0 until 2).map(i => udfSafeString(col(\"temp\").getItem(i)).as(s\"nbPassengers$i\")): _*\n        ).withColumn(\"temp\", col(\"bookingStatus\")).select(\n            col(\"*\") +:(0 until 2).map(i => udfSafeString(col(\"temp\").getItem(i)).as(s\"bookingStatus$i\")): _*\n        ).withColumn(\"temp\", col(\"operatingAirline\")).select(\n             col(\"*\") +:(0 until 2).map(i => udfSafeString(col(\"temp\").getItem(i)).as(s\"operatingAirline$i\")): _*\n        ).withColumn(\"temp\", col(\"originAirport\")).select(\n             col(\"*\") +:(0 until 2).map(i => udfSafeString(col(\"temp\").getItem(i)).as(s\"originAirport$i\")): _*\n        ).withColumn(\"temp\", col(\"departureDate\")).select(\n             col(\"*\") +:(0 until 2).map(i => udfSafeString(col(\"temp\").getItem(i)).as(s\"departureDate$i\")): _*\n        ).withColumn(\"temp\", col(\"arrivalDate\")).select(\n             col(\"*\") +:(0 until 2).map(i => udfSafeString(col(\"temp\").getItem(i)).as(s\"arrivalDate$i\")): _*\n        ).withColumn(\"temp\", col(\"destinationAirport\")).select(\n             col(\"nbPassengers0\") +:col(\"nbPassengers1\") +:col(\"bookingStatus0\") +:col(\"bookingStatus1\") +:col(\"operatingAirline0\") +:col(\"operatingAirline1\") +:col(\"originAirport0\") +:\n             col(\"originAirport1\") +:col(\"departureDate0\") +:col(\"departureDate1\") +:col(\"arrivalDate0\") +:col(\"arrivalDate1\")\n              +:(0 until 2).map(i => udfSafeString(col(\"temp\").getItem(i)).as(s\"destinationAirport$i\")): _*)\n\n\n//for production enviroment\n// val query = booking\n//               .writeStream\n//               .format(\"com.databricks.spark.avro\")\n//               .option(\"path\", \"/user/maria_dev/output\")\n//               .option(\"checkpointLocation\", \"/tmp/\")\n//              .start()\n//query.awaitTermination()\n\n    val query = booking.writeStream\n          .outputMode(\"append\")\n          .queryName(\"bookingLoading\")\n          .format(\"console\")\n          .start()\n\n    query.awaitTermination()\n\n     //Break each two ways booking to 2 separate rows\n        val finalDF = booking\n          .rdd\n          .flatMap(row => {\n            val nbPassengers0 = row.getString(0)\n            val bookingStatus0 = row.getString(2).toLowerCase\n            val operatingAirline0 = row.getString(4).toLowerCase\n            val originAirport0 = row.getString(6).toLowerCase\n            val departureDate0 = row.getString(8)\n            val arrivalDate0 = row.getString(10)\n            val destinationAirport0 = row.getString(12).toLowerCase\n            \n        \n            val nbPassengers1 = row.getString(1)\n            val bookingStatus1 = row.getString(3).toLowerCase\n            val operatingAirline1 = row.getString(5).toLowerCase\n            val originAirport1 = row.getString(7).toLowerCase\n            val departureDate1 = row.getString(9)\n            val arrivalDate1 = row.getString(11)\n            val destinationAirport1 = row.getString(13).toLowerCase\n    \n        \n            List(\n              (nbPassengers0, operatingAirline0, bookingStatus0, originAirport0, destinationAirport0, departureDate0, arrivalDate0),\n              (nbPassengers1, operatingAirline1, bookingStatus1, originAirport1, destinationAirport1, departureDate1, arrivalDate1)\n            )\n          })\n          .toDF(\"nbPassengers\", \"operatingAirline\", \"bookingStatus\", \"originAirport\", \"destinationAirport\", \"departureDate\", \"arrivalDate\")\n        //finalDF.show()\n        return finalDF;\n}\n  \n\n","user":"anonymous","dateUpdated":"2019-07-11T15:42:30+0000","config":{"colWidth":12,"fontSize":9,"enabled":false,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1562712416496_-1575482747","id":"20190709-224656_11245013","dateCreated":"2019-07-09T22:46:56+0000","dateStarted":"2019-07-11T13:50:54+0000","dateFinished":"2019-07-11T13:50:27+0000","status":"ABORT","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:589"}],"name":"KLM/BookingStreaming","id":"2EFRT9GUA","noteParams":{},"noteForms":{},"angularObjects":{"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}